{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2455b14",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "### Q1\n",
    "\n",
    "1. Modify the code I wrote in lecture 8 with what you have learnt in lecture 9 and correctly tokenize the text both on the word and sentence level, and by removing the stopwords. Rewrite the `getSummary` function and all the other functions that it depends by maing these corrections.\n",
    "\n",
    "2. Rewrite the code I wrote for `getKeywords` function making the same corrections.\n",
    "\n",
    "3. Test your code from parts 1 and 2 on random articles from the Guardian.\n",
    "\n",
    "4. Rewrite the `getSubjectGuardian` function for another newspaper in English, and test your code from part 1 and 2 on random articles from this new newspaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f6b62",
   "metadata": {},
   "source": [
    "# Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad6458",
   "metadata": {},
   "source": [
    "## Solution 1.1\n",
    "All the libraries included are the ones used during the lecture. It would be a waste of space to explain them all, so I decided not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2a01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from xmltodict import parse\n",
    "import regex as re\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf77ba0",
   "metadata": {},
   "source": [
    "The set of stop words of english was declared globally since it was going to be used occasionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sten = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3531f8",
   "metadata": {},
   "source": [
    "`getBodies` function takes a subject as its argument and returns the list of texts of the subjects' news in RSS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288466c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBodies(subject):\n",
    "    with requests.get(f\"https://www.theguardian.com/uk/{subject}/rss\") as url:\n",
    "        raw = parse(url.text)\n",
    "    bodies = []\n",
    "    for i in range(len(raw[\"rss\"][\"channel\"][\"item\"])):\n",
    "        link = raw[\"rss\"][\"channel\"][\"item\"][i][\"link\"]\n",
    "        with requests.get(link) as url:\n",
    "            body = BeautifulSoup(url.content, \"html.parser\")\n",
    "        bodies.append(\" \".join([x.text for x in body.find_all(\"p\")]))        \n",
    "    return bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3996c92",
   "metadata": {},
   "source": [
    "`tokenizer` function takes any text as its argument and then splits it into sentences and words via the built-in `_tokenize` function. It then returns a dictionary with split sentences and words as its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07f1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(article):\n",
    "    tokenized = {\"sentences\":sent_tokenize(article), \"words\":word_tokenize(article)}\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80be51",
   "metadata": {},
   "source": [
    "`cleaner` function takes the tokenized text as its argument and removes the numerals, punctuation, and stop words and also transforms all the letters into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b646d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tokenized):\n",
    "    tokenized.update({\"cleanedSentences\":[re.sub(r'[^\\p{Letter}\\s]', \"\", sentence.lower()) for sentence in tokenized[\"sentences\"]]})\n",
    "    tokenized.update({\"cleanedWords\": [re.sub(r'[^\\p{Letter}]', \"\", word.lower()) for word in tokenized[\"words\"]]})\n",
    "    tokenized.update({\"woswwords\":[x for x in tokenized[\"cleanedWords\"] if x not in sten and x !=\"\"]})\n",
    "    woswsent = []\n",
    "    for x in tokenized[\"cleanedSentences\"]:\n",
    "        tmp = word_tokenize(x)\n",
    "        tmp = [t for t in tmp if t not in sten]\n",
    "        tmp = \" \".join(tmp)\n",
    "        woswsent.append(tmp)\n",
    "    tokenized.update({\"woswsent\": woswsent})\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ee8a1",
   "metadata": {},
   "source": [
    "`getMatrix` function is as used in the lecture. It basically vectorizes the text with respect to distinct words' count in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a77d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93958808",
   "metadata": {},
   "source": [
    "`getSummary` function is also exactly as used in the lecture. It vectorizes the text, establishes a weight for each sentence and sorts the sentences with respect to their weights and returns the most important _k_ sentences in the text with their indices intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e806b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(textlist, k):\n",
    "    projection = PCA(n_components = 1)\n",
    "    techMatrix = getMatrix(textlist[0])\n",
    "    weights = projection.fit_transform(techMatrix.toarray())\n",
    "    res = list(zip(weights.transpose()[0], range(len(textlist[0])), textlist[1]))\n",
    "    ret = sorted(res, key = lambda x:x[0], reverse = True)[:k]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e1f3c",
   "metadata": {},
   "source": [
    "## Solution 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ff446",
   "metadata": {},
   "source": [
    "`getKeywords` function too is nearly the same as the one used in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2956f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywords(text, k):\n",
    "    vectorizer = CountVectorizer(stop_words=sten)\n",
    "    matrix = vectorizer.fit_transform(text[0])\n",
    "    words = vectorizer.get_feature_names()\n",
    "    projection = PCA(n_components = 1)\n",
    "    tmp  = projection.fit_transform(matrix.transpose().toarray())\n",
    "    weights = tmp.transpose()[0]\n",
    "    return sorted(zip(weights, words), key = lambda x: x[0], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b30edf",
   "metadata": {},
   "source": [
    "## Solution 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62c5ab",
   "metadata": {},
   "source": [
    "The \"Technology\" branch of the Guardian's feed was used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f530d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = getBodies(\"technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe755c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.6930979498391507,\n",
       "  1,\n",
       "  'heres how garmin connect and express have been taken offline by a reported ransomware attack leaving runners cyclists walkers and others unable to sync their activities to strava'),\n",
       " (0.9004850526553444,\n",
       "  0,\n",
       "  'garmin servers are offline but you can still share your runs rides swims and walks with strava'),\n",
       " (0.7478077309043233,\n",
       "  2,\n",
       "  'but dont worry  there is a manual way to upload your activities to strava while garmin is down'),\n",
       " (0.21896215768647403,\n",
       "  3,\n",
       "  'heres how what you need your garmin watch or cycling computer'),\n",
       " (0.1556044434480564,\n",
       "  7,\n",
       "  'connect your garmin device to your computer with the usb cable and wait for it to be recognised like a standard flash drive or memory stick')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.random.randint(0, len(tech))\n",
    "techcl = [cleaner(tokenizer(tech[m]))[\"woswsent\"], cleaner(tokenizer(tech[m]))[\"cleanedSentences\"]]\n",
    "getSummary(techcl, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c358b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.6118594900961734, 'garmin'),\n",
       " (0.7804976773732376, 'strava'),\n",
       " (0.7795358772199774, 'connect'),\n",
       " (0.6730515001828693, 'device'),\n",
       " (0.5966073612078563, 'heres')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKeywords(techcl, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9e1c0",
   "metadata": {},
   "source": [
    "## Solution 1.4\n",
    "United Nations News' RSS feed was used as a second source to parse. `getSubject` function was created to fork the news off the feed. It takes a subject as its argument. Check out the [Feed](https://news.un.org/en/rss-feeds) to see the topics:\n",
    "* Health\n",
    "* UN-Affairs\n",
    "* Law-and-Crime-Prevention\n",
    "* Human-Rights\n",
    "* Humanitarian-Aid\n",
    "* Climate-Change\n",
    "* Culture-and-Education\n",
    "* Economic-Development\n",
    "* Women\n",
    "* Peace-and-Security\n",
    "* Migrants-and-Refugees\n",
    "* SDGs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122733bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubject(subject):\n",
    "    with requests.get(f\"https://news.un.org/feed/subscribe/en/news/topic/{subject}/feed/rss.xml\") as url:\n",
    "        raw = parse(url.text)\n",
    "    bodies = []\n",
    "    for x in range(len(raw[\"rss\"][\"channel\"])):\n",
    "            ind_link = raw[\"rss\"][\"channel\"][\"item\"][x][\"link\"]\n",
    "            with requests.get(ind_link) as link:\n",
    "                body = BeautifulSoup(link.content)\n",
    "            bodies.append(\" \".join([x.text for x in body.find_all(\"p\")]))\n",
    "    return bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b044f8",
   "metadata": {},
   "source": [
    "**EXECUTE THE GETSUMMARY AND GETKEYWORDS FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a4e89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealth = getSubject(\"Health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26aaece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.4288066914466837,\n",
       "  2,\n",
       "  'working closely with health and disaster management agencies the national meteorological and hydrological departments in both countries plan to roll out heat health action plans which have been successful in saving lives in the past few years said the un weather agency in a statement'),\n",
       " (3.3498517354965784,\n",
       "  20,\n",
       "  'india has established a national framework for heat action plans through the national disaster management authority which coordinates a network of state disaster response agencies and city leaders to prepare for soaring temperatures and ensure that everyone is aware of heatwave protocols'),\n",
       " (2.177969497514947,\n",
       "  0,\n",
       "  'subscribe audio hub with extreme heat gripping large parts of india and pakistan the two countries are working to roll out lifesaving health action plans to combat the heatwave the world meteorological organization wmo said on friday'),\n",
       " (0.7170508674487965,\n",
       "  21,\n",
       "  'the city of ahmedabad in india was the first south asian city to develop and implement a citywide heat health adaptation in  after experiencing a devastating heatwave in '),\n",
       " (0.24536589753339255,\n",
       "  6,\n",
       "  'both india and pakistan have successful heathealth early warning systems and action plans already in place including those specially tailored for urban areas')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.random.randint(0, len(unhealth))\n",
    "clunhealth = [cleaner(tokenizer(unhealth[t]))[\"woswsent\"], cleaner(tokenizer(unhealth[t]))[\"cleanedSentences\"]]\n",
    "getSummary(clunhealth, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c5ebae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.2509815533181396, 'heat'),\n",
       " (1.8448167782660871, 'health'),\n",
       " (1.6177770471984427, 'india'),\n",
       " (1.4491089222017326, 'action'),\n",
       " (1.4183724758333833, 'pakistan')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKeywords(clunhealth, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148a291",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Write a function that returns all named entities (proper names, country names, corporation names only) from a URL. Function should take the URL as the input and must return the list of named entities from that URL. Test your code on random articles from the Guardian. Don't use the NLTK's NER that I demonstrated during the lecture. Use the SpaCY's NER function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba4236",
   "metadata": {},
   "source": [
    "# Solution 2\n",
    "Import `spacy` to use its built-in Named Entity Recognizer function. `BeautifulSoup` and `requests` were also used in the function (_BS_ to parse the page and _requests_ to make a web connection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a6c7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b62bb",
   "metadata": {},
   "source": [
    "The `identifier` function takes a URL as its argument, parses it, finds the body text, and merges the text into one paragraph. Then `spacy`s built-in english model was called to analyse the paragraph and spot the named entities. The names of the entities and the semantic types of them are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ccfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifier(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content, \"html.parser\")\n",
    "    body = \" \".join([x.text for x in raw.find_all(\"p\")])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    textus = nlp(body)\n",
    "    names = []\n",
    "    constraint = [\"GPE\", \"PERSON\", \"ORG\"]\n",
    "    [names.append((x.text, spacy.explain(x.label_))) for x in textus.ents if x.label_ in constraint]\n",
    "    names = set(names)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4ee7d",
   "metadata": {},
   "source": [
    "**EXAMPLE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79e41e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Auschwitz', 'People, including fictional'),\n",
       " ('Boris Johnson', 'People, including fictional'),\n",
       " ('Brexit', 'People, including fictional'),\n",
       " ('Brexiters', 'Companies, agencies, institutions, etc.'),\n",
       " ('Britain', 'Countries, cities, states'),\n",
       " ('Britons', 'People, including fictional'),\n",
       " ('Channel', 'Companies, agencies, institutions, etc.'),\n",
       " ('EU', 'Companies, agencies, institutions, etc.'),\n",
       " ('EU', 'Countries, cities, states'),\n",
       " ('Eurotunnel', 'Companies, agencies, institutions, etc.'),\n",
       " ('Guardian', 'Companies, agencies, institutions, etc.'),\n",
       " ('Jacob Rees-Mogg', 'People, including fictional'),\n",
       " ('Johnson', 'People, including fictional'),\n",
       " ('Johnson’s', 'Companies, agencies, institutions, etc.'),\n",
       " ('Join Jonathan Freedland', 'People, including fictional'),\n",
       " ('Jonathan Freedland', 'People, including fictional'),\n",
       " ('Marie Antoinette', 'People, including fictional'),\n",
       " ('Michael Kinsley', 'People, including fictional'),\n",
       " ('NHS', 'Companies, agencies, institutions, etc.'),\n",
       " ('Partygate', 'Companies, agencies, institutions, etc.'),\n",
       " ('Rees-Mogg', 'Companies, agencies, institutions, etc.'),\n",
       " ('Rees-Mogg’s', 'Companies, agencies, institutions, etc.'),\n",
       " ('Ukraine', 'Countries, cities, states'),\n",
       " ('Washington', 'Countries, cities, states'),\n",
       " ('the British Veterinary Association',\n",
       "  'Companies, agencies, institutions, etc.'),\n",
       " ('the UK Major Ports Group', 'Companies, agencies, institutions, etc.')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1 = \"https://www.theguardian.com/commentisfree/2022/apr/29/jacob-rees-mogg-brexit-disaster-leaving-eu-boris-johnson\"\n",
    "identifier(url1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a2bf3",
   "metadata": {},
   "source": [
    "**EXAMPLE 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f46ee0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ADHD Foundation', 'Companies, agencies, institutions, etc.'),\n",
       " ('Blimey', 'People, including fictional'),\n",
       " ('Olivia Attwood –', 'People, including fictional'),\n",
       " ('Solange Knowles', 'People, including fictional'),\n",
       " ('Tony', 'People, including fictional'),\n",
       " ('Tony Lloyd', 'People, including fictional')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://www.theguardian.com/lifeandstyle/2022/apr/29/could-i-have-undiagnosed-adhd-we-ask-an-expert\"\n",
    "identifier(url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00a5e3",
   "metadata": {},
   "source": [
    "**EXAMPLE 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f15b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Becker', 'Companies, agencies, institutions, etc.'),\n",
       " ('Becker', 'People, including fictional'),\n",
       " ('Boris Becker', 'People, including fictional'),\n",
       " ('Boris Becker’s', 'People, including fictional'),\n",
       " ('Dean Beale', 'People, including fictional'),\n",
       " ('Deborah Taylor', 'People, including fictional'),\n",
       " ('Germany', 'Countries, cities, states'),\n",
       " ('Jonathan Laidlaw', 'People, including fictional'),\n",
       " ('Leimen', 'Countries, cities, states'),\n",
       " ('Lilian de Carvalho Monteiro', 'Companies, agencies, institutions, etc.'),\n",
       " ('Matthew Carter', 'People, including fictional'),\n",
       " ('Mazars', 'Companies, agencies, institutions, etc.'),\n",
       " ('Rebecca Chalkley', 'People, including fictional'),\n",
       " ('Sentencing Becker', 'People, including fictional'),\n",
       " ('Southwark', 'Companies, agencies, institutions, etc.'),\n",
       " ('the Insolvency Service', 'Companies, agencies, institutions, etc.'),\n",
       " ('the National Bankruptcy Centre', 'Companies, agencies, institutions, etc.')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url3 = \"https://www.theguardian.com/uk-news/2022/apr/29/boris-becker-jailed-two-years-for-hiding-assets-after-bankruptcy\"\n",
    "identifier(url3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d80aaa",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "1. Write a function that returns the most positive and the most negative sentences from a text. The function must take the text as the input and must return a 2-tuple: the first element as the most positive and the second as the most negative sentence with their polarity scores.\n",
    "\n",
    "2. Test your function on random articles from the Guardian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215b34f",
   "metadata": {},
   "source": [
    "# Solution 3\n",
    "## Solution 3.1\n",
    "\n",
    "`nltk`s `SentimentIntensityAnalyzer` was imported to measure the mood of the sentences. A short function `textExtractor` was first introduced to provide text for the main `polarity` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12499879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b358a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textExtractor(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content, \"html.parser\")\n",
    "    body = \" \".join([x.text for x in raw.find_all(\"p\")])\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb53bdd",
   "metadata": {},
   "source": [
    "The text is then fed into the `polarity` function. The function first tokenizes the text by sentences and then measures each sentence's mood via the `analyzer`. The sentences and their scores are then appended to a list `scores`, and also the compound score of each sentence was appended to a separate list. The seperate list allows us to locate the most positive and the most negative sentences easily. The function returns the most positive and the most negative sentences with their respectful scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "190d9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    tmp = sent_tokenize(text)\n",
    "    scores = [(x, analyzer.polarity_scores(x)) for x in tmp]\n",
    "    s = [analyzer.polarity_scores(x)[\"compound\"] for x in tmp]\n",
    "    \n",
    "    positive = [x for x in scores if x[1][\"compound\"] == max(s)]\n",
    "    negative = [x for x in scores if x[1][\"compound\"] == min(s)]\n",
    "    positive = positive[0]\n",
    "    negative = negative[0]\n",
    "    \n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796d7c5",
   "metadata": {},
   "source": [
    "## Solution 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b49602",
   "metadata": {},
   "source": [
    "**EXAMPLE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3377cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('In earlier comments, Musk has been outspoken about his desire to promote free speech on Twitter, saying that he is “against censorship that goes far beyond the law”.',\n",
       "  {'neg': 0.0, 'neu': 0.744, 'pos': 0.256, 'compound': 0.8225}),\n",
       " ('His tweets sparked tens of thousands of abusive messages targeted at the executive, and a public rebuke from a former Twitter chief executive, Dick Costolo.',\n",
       "  {'neg': 0.263, 'neu': 0.737, 'pos': 0.0, 'compound': -0.8176}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1 = \"https://www.theguardian.com/technology/2022/apr/28/elon-musk-says-twitter-must-be-politically-neutral-as-some-leftwing-users-quit\"\n",
    "text1 = textExtractor(url1)\n",
    "polarity(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c08d6",
   "metadata": {},
   "source": [
    "**EXAMPLE 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a9e4358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('We can make great listeners, and great friends, because we understand others.',\n",
       "  {'neg': 0.0, 'neu': 0.443, 'pos': 0.557, 'compound': 0.9062}),\n",
       " ('If you’re angry about animal cruelty, volunteer as a dog walker at your local animal shelter (there is always a need); if the report of a serious road accident upsets you, write to your local council about speed cameras.',\n",
       "  {'neg': 0.313, 'neu': 0.687, 'pos': 0.0, 'compound': -0.9201}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://www.theguardian.com/lifeandstyle/2022/apr/29/confessions-of-a-hyper-empath\"\n",
    "text2 = textExtractor(url2)\n",
    "polarity(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f24ca",
   "metadata": {},
   "source": [
    "**EXAMPLE 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2756d853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Didn’t obviously turn out that way but I have so much love for ‘Tae and appreciate the time we spent together and definitely wish him the best in Derek [Carr] in Vegas.',\n",
       "  {'neg': 0.0, 'neu': 0.534, 'pos': 0.466, 'compound': 0.9768}),\n",
       " ('Before that, the Packers hadn’t chosen an offensive player in the first round since taking Mississippi State tackle Derek Sherrod 32nd overall in 2011.',\n",
       "  {'neg': 0.115, 'neu': 0.885, 'pos': 0.0, 'compound': -0.4588}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url3 = \"https://www.theguardian.com/sport/2022/apr/29/green-bay-packers-aaron-rodgers-davante-adams\"\n",
    "text3 = textExtractor(url3)\n",
    "polarity(text3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
